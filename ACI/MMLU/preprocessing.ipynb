{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b12f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 从环境变量读取 OpenAI API 密钥\n",
    "# 密钥应该已经在 ~/.bashrc 中设置，或者通过 export OPENAI_API_KEY=... 设置\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY 环境变量未设置。请运行以下命令之一：\\n\"\n",
    "        \"1. export OPENAI_API_KEY=\\your-api-key'\\n\"\n",
    "        \"2. 或者在 ~/.bashrc 中添加: export OPENAI_API_KEY=\\your-api-key' 然后重启终端\"\n",
    "    )\n",
    "print(\"✓ OpenAI API 密钥已从环境变量加载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23b7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai transformers datasets accelerate torch pandas pyarrow tqdm\n",
    "# 'accelerate' 是为了更快地加载和运行模型\n",
    "# 'pyarrow' 是为了将 DataFrame 保存为 parquet 格式\n",
    "# 'openai' 用于 GPT-3.5 API\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 检查是否有可用的 GPU (在 Colab 或本地)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 初始化 OpenAI 客户端 (需要设置 OPENAI_API_KEY 环境变量)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be85c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-3.5-turbo via OpenAI API\n",
      "Model ready to use.\n"
     ]
    }
   ],
   "source": [
    "# 使用 GPT-3.5 Turbo\n",
    "MODEL_ID = \"gpt-3.5-turbo\"\n",
    "\n",
    "# 注意：GPT-3.5 通过 OpenAI API 使用，不需要本地加载模型\n",
    "# 确保已设置 OPENAI_API_KEY 环境变量\n",
    "print(f\"Using model: {MODEL_ID} via OpenAI API\")\n",
    "print(\"Model ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90af7d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all available MMLU subjects...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 subjects in MMLU dataset\n",
      "After filtering out 'all', 58 subjects remain\n",
      "Selected first 20 subjects for processing:\n",
      "Subjects: ['abstract_algebra', 'anatomy', 'astronomy', 'auxiliary_train', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology']\n",
      "Loaded 100 questions for subject: abstract_algebra\n",
      "Loaded 135 questions for subject: anatomy\n",
      "Loaded 152 questions for subject: astronomy\n",
      "Failed to load auxiliary_train: Unknown split \"test\". Should be one of ['train'].\n",
      "Loaded 100 questions for subject: business_ethics\n",
      "Loaded 265 questions for subject: clinical_knowledge\n",
      "Loaded 144 questions for subject: college_biology\n",
      "Loaded 100 questions for subject: college_chemistry\n",
      "Loaded 100 questions for subject: college_computer_science\n",
      "Loaded 100 questions for subject: college_mathematics\n",
      "Loaded 173 questions for subject: college_medicine\n",
      "Loaded 102 questions for subject: college_physics\n",
      "Loaded 100 questions for subject: computer_security\n",
      "Loaded 235 questions for subject: conceptual_physics\n",
      "Loaded 114 questions for subject: econometrics\n",
      "Loaded 145 questions for subject: electrical_engineering\n",
      "Loaded 378 questions for subject: elementary_mathematics\n",
      "Loaded 126 questions for subject: formal_logic\n",
      "Loaded 100 questions for subject: global_facts\n",
      "Loaded 310 questions for subject: high_school_biology\n",
      "\n",
      "Successfully loaded 19 subjects out of 20 selected subjects\n"
     ]
    }
   ],
   "source": [
    "# 获取 MMLU 数据集的所有可用主题\n",
    "# 先获取所有主题，排除 'all'，然后选择前20个进行标注\n",
    "print(\"Fetching all available MMLU subjects...\")\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "# 获取所有可用的配置（主题）\n",
    "all_subjects = get_dataset_config_names(\"cais/mmlu\")\n",
    "print(f\"Found {len(all_subjects)} subjects in MMLU dataset\")\n",
    "\n",
    "# 排除 'all' 这个subject\n",
    "filtered_subjects = [s for s in all_subjects if s != 'all']\n",
    "print(f\"After filtering out 'all', {len(filtered_subjects)} subjects remain\")\n",
    "\n",
    "# 只选择前20个subject进行标注\n",
    "SUBJECTS = filtered_subjects[:20]\n",
    "print(f\"Selected first {len(SUBJECTS)} subjects for processing:\")\n",
    "print(f\"Subjects: {SUBJECTS}\")\n",
    "\n",
    "# 用于存储所有加载数据的字典\n",
    "mmlu_data = {}\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    # MMLU 的 \"test\" 集是有标签的，\"validation\" 集是无标签的（用于官方提交）\n",
    "    # 所以我们加载 \"test\" 集\n",
    "    try:\n",
    "        dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
    "        mmlu_data[subject] = dataset\n",
    "        print(f\"Loaded {len(dataset)} questions for subject: {subject}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {subject}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(mmlu_data)} subjects out of {len(SUBJECTS)} selected subjects\")\n",
    "\n",
    "# MMLU 的选项\n",
    "CHOICES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e71f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mmlu_prompt(sample, subject_name):\n",
    "    \"\"\"\n",
    "    将 MMLU 的一行数据格式化为 zero-shot CoT prompt。\n",
    "    \"\"\"\n",
    "    subject_formatted = subject_name.replace(\"_\", \" \")\n",
    "    question = sample['question']\n",
    "    \n",
    "    # 组合选项\n",
    "    options = \"\"\n",
    "    for i, choice in enumerate(sample['choices']):\n",
    "        options += f\"{CHOICES[i]}. {choice}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"The following is a multiple-choice question about {subject_formatted}. Please choose the single most likely answer.\n",
    "\n",
    "Question: {question}\n",
    "{options}\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_choice_probabilities(prompt, model_id, client, num_choices=None):\n",
    "    \"\"\"\n",
    "    给定一个 prompt，计算模型对选项的概率。\n",
    "    使用 OpenAI API 的 logprobs 功能。\n",
    "    \n",
    "    Args:\n",
    "        prompt: 输入提示\n",
    "        model_id: 模型ID\n",
    "        client: OpenAI客户端\n",
    "        num_choices: 选项数量（如果为None，则使用CHOICES的长度）\n",
    "    \"\"\"\n",
    "    # 确定实际使用的选项数量\n",
    "    if num_choices is None:\n",
    "        num_choices = len(CHOICES)\n",
    "    actual_choices = CHOICES[:num_choices]\n",
    "    \n",
    "    # 1. 准备选项 token (GPT-3.5 通常使用 \" A\", \" B\", \" C\", \" D\" 等格式)\n",
    "    choice_tokens = [f\" {choice}\" for choice in actual_choices]\n",
    "    \n",
    "    # 2. 调用 OpenAI API 获取 logprobs (带重试机制)\n",
    "    import time\n",
    "    max_retries = 3\n",
    "    retry_delay = 1  # 初始延迟（秒）\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                logprobs=True,  # 启用 logprobs\n",
    "                top_logprobs=20,  # 获取 top 20 的 logprobs\n",
    "                max_tokens=1,  # 只生成一个 token\n",
    "                temperature=0  # 使用确定性输出\n",
    "            )\n",
    "            break  # 成功则跳出重试循环\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                # 如果是速率限制错误，等待更长时间\n",
    "                if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                    wait_time = retry_delay * (2 ** attempt)  # 指数退避\n",
    "                    print(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    time.sleep(retry_delay * (2 ** attempt))\n",
    "                continue\n",
    "            else:\n",
    "                # 最后一次尝试也失败，返回均匀分布\n",
    "                print(f\"Error calling OpenAI API after {max_retries} attempts: {e}\")\n",
    "                return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    if response is None:\n",
    "        return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    # 3. 获取第一个（也是唯一的）token 的 logprobs\n",
    "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
    "        token_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "        # 创建一个字典，将 token 文本映射到 logprob\n",
    "        logprob_dict = {item.token: item.logprob for item in token_logprobs}\n",
    "    else:\n",
    "        logprob_dict = {}\n",
    "    \n",
    "    # 4. 提取每个选项的 logprob\n",
    "    choice_logprobs = []\n",
    "    for choice_token in choice_tokens:\n",
    "        choice_letter = choice_token.strip()  # 获取字母部分 (A, B, C, D, E, F)\n",
    "        logprob = None\n",
    "        \n",
    "        # 尝试多种可能的 token 格式\n",
    "        # 1. 带前导空格的格式: \" A\", \" B\", \" C\", etc.\n",
    "        # 注意: choice_token 已经是 \" A\" 格式，所以直接使用\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(choice_token, None)\n",
    "        \n",
    "        # 2. 不带空格的格式: \"A\", \"B\", \"C\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(choice_letter, None)\n",
    "        \n",
    "        # 3. 带点号的格式: \"A.\", \"B.\", \"C.\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(f\"{choice_letter}.\", None)\n",
    "        \n",
    "        # 4. 带前导空格和点号的格式: \" A.\", \" B.\", \" C.\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(f\" {choice_letter}.\", None)\n",
    "        \n",
    "        # 5. 规范化匹配：去除所有空格和标点后比较\n",
    "        if logprob is None:\n",
    "            for token, lp in logprob_dict.items():\n",
    "                # 规范化 token：去除空格、点号等，只保留字母\n",
    "                normalized_token = ''.join(c for c in token if c.isalpha())\n",
    "                if normalized_token == choice_letter:\n",
    "                    logprob = lp\n",
    "                    break\n",
    "        \n",
    "        # 6. 大小写不敏感匹配\n",
    "        if logprob is None:\n",
    "            for token, lp in logprob_dict.items():\n",
    "                normalized_token = ''.join(c for c in token if c.isalpha())\n",
    "                if normalized_token.upper() == choice_letter.upper():\n",
    "                    logprob = lp\n",
    "                    break\n",
    "        \n",
    "        if logprob is None:\n",
    "            # 如果找不到，使用一个很小的值\n",
    "            logprob = -100.0\n",
    "            # 只在第一次找不到时打印调试信息\n",
    "            if len(choice_logprobs) == 0:  # 只在第一个选项找不到时打印\n",
    "                print(f\"Warning: Could not find logprob for choice token '{choice_token}'\")\n",
    "                print(f\"Available tokens (top 10): {list(logprob_dict.keys())[:10]}\")\n",
    "        \n",
    "        choice_logprobs.append(logprob)\n",
    "    \n",
    "    # 5. 将 logprobs 转换为 logits (logprobs 已经是 log 概率)\n",
    "    choice_logits = np.array(choice_logprobs)\n",
    "    \n",
    "    # 6. 应用 softmax 得到概率分布\n",
    "    # 为了避免数值不稳定，减去最大值\n",
    "    choice_logits_shifted = choice_logits - np.max(choice_logits)\n",
    "    exp_logits = np.exp(choice_logits_shifted)\n",
    "    choice_probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    return choice_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279e44e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test S(A): 0.5800000000000001\n",
      "Test S(B): 0.17999999999999994\n",
      "Test S(C): 0.07999999999999996\n",
      "Test S(D): 0.029999999999999916\n",
      "Test S(E): 0.009999999999999898\n",
      "Test S(F): 0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_aps_score(probs, choice_index):\n",
    "    \"\"\"\n",
    "    为 *一个* 假设的答案 (choice_index) 计算 APS 不一致性分数。\n",
    "    S(X, y) = 1 - (所有 P_j >= P_y 的 P_j 的总和)\n",
    "    \n",
    "    Args:\n",
    "    - probs (np.array): 概率数组，例如 [P(A), P(B), P(C), P(D), P(E), P(F)]\n",
    "    - choice_index (int): 我们正在计算分数的那个选项 (0=A, 1=B, 2=C, 3=D, 4=E, 5=F)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 获取我们正在打分的这个选项的概率\n",
    "    prob_y = probs[choice_index]\n",
    "    \n",
    "    # 2. 找到所有概率 >= prob_y 的选项\n",
    "    indices_to_sum = np.where(probs >= prob_y)[0]\n",
    "    \n",
    "    # 3. 把它们的概率加起来\n",
    "    # 为了处理浮点数精度问题，我们应该比较 probs >= prob_y - 1e-9\n",
    "    prob_sum = 0\n",
    "    for idx in indices_to_sum:\n",
    "        # 再次检查，避免浮点数问题\n",
    "        if probs[idx] >= prob_y - 1e-9:\n",
    "            prob_sum += probs[idx]\n",
    "            \n",
    "    # 4. APS 分数\n",
    "    score = 1.0 - prob_sum\n",
    "    \n",
    "    return score\n",
    "\n",
    "# --- 快速测试一下我们的计分函数 ---\n",
    "test_probs = np.array([0.42, 0.40, 0.10, 0.05, 0.02, 0.01])\n",
    "# 选项 A: S(A) = 1 - P(A) = 1 - 0.45 = 0.55\n",
    "# 选项 B: S(B) = 1 - (P(A) + P(B)) = 1 - (0.45 + 0.40) = 0.15\n",
    "# 选项 C: S(C) = 1 - (P(A) + P(B) + P(C)) = 1 - (0.45 + 0.40 + 0.10) = 0.05\n",
    "# 选项 D: S(D) = 1 - (P(A) + P(B) + P(C) + P(D)) = 1 - 1.0 = 0.0\n",
    "\n",
    "print(f\"Test S(A): {calculate_aps_score(test_probs, 0)}\") # 应该约等于 0.55\n",
    "print(f\"Test S(B): {calculate_aps_score(test_probs, 1)}\") # 应该约等于 0.15\n",
    "print(f\"Test S(C): {calculate_aps_score(test_probs, 2)}\") # 应该约等于 0.05\n",
    "print(f\"Test S(D): {calculate_aps_score(test_probs, 3)}\") # 应该约等于 0.0\n",
    "print(f\"Test S(E): {calculate_aps_score(test_probs, 4)}\") # 应该约等于 0.02\n",
    "print(f\"Test S(F): {calculate_aps_score(test_probs, 5)}\") # 应该约等于 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject: abstract_algebra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [00:45<00:02,  2.50it/s]"
     ]
    }
   ],
   "source": [
    "results_list = [] # 用于存储我们所有数据的列表\n",
    "\n",
    "# 遍历我们加载的每个 MMLU 主题\n",
    "for subject_name, dataset in mmlu_data.items():\n",
    "    print(f\"\\nProcessing subject: {subject_name}...\")\n",
    "    \n",
    "    # 遍历该主题中的所有问题\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        \n",
    "        # 1. 格式化 prompt\n",
    "        prompt = format_mmlu_prompt(sample, subject_name)\n",
    "        \n",
    "        # 2. 获取实际选项数量（MMLU问题可能有不同数量的选项）\n",
    "        num_choices = len(sample['choices'])\n",
    "        \n",
    "        # 3. 获取概率分布 [P(A), P(B), P(C), ...]\n",
    "        try:\n",
    "            probabilities = get_choice_probabilities(prompt, MODEL_ID, client, num_choices=num_choices)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # 4. 获取标准答案\n",
    "        ground_truth_label = sample['answer'] # 这是一个 0 到 num_choices-1 的索引\n",
    "        \n",
    "        # 5. 为 *每一个* 选项计算 APS 分数\n",
    "        for j in range(num_choices):\n",
    "            \n",
    "            aps_score = calculate_aps_score(probabilities, j)\n",
    "            \n",
    "            # 6. 结构化保存\n",
    "            row = {\n",
    "                \"question_id\": f\"{subject_name}_{i}\",\n",
    "                \"subject\": subject_name,\n",
    "                \"question\": sample['question'],\n",
    "                \"choice_str\": CHOICES[j],          # A, B, C, D, E, or F\n",
    "                \"choice_index\": j,\n",
    "                \"choice_text\": sample['choices'][j],\n",
    "                \"probability\": probabilities[j],   # 模型对这个选项的原始概率\n",
    "                \"aps_score\": aps_score,            # 这个选项的 APS 不一致性分数\n",
    "                \"is_ground_truth\": (j == ground_truth_label) # 这是一个 bool 值\n",
    "            }\n",
    "            results_list.append(row)\n",
    "\n",
    "print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: PyArrow filesystem registry error. Saving as CSV instead: /egr/research-hintlab/liuxin73/projects/conformal-factual-lm/ACI/MMLU/mmlu_with_aps_scores.csv\n",
      "Successfully processed 4088 rows (1022 questions).\n",
      "Average number of choices per question: 4.0\n",
      "Data saved to /egr/research-hintlab/liuxin73/projects/conformal-factual-lm/ACI/MMLU/mmlu_with_aps_scores.csv\n",
      "\n",
      "--- DataFrame Head ---\n",
      "                question_id                 subject  \\\n",
      "0  high_school_us_history_0  high_school_us_history   \n",
      "1  high_school_us_history_0  high_school_us_history   \n",
      "2  high_school_us_history_0  high_school_us_history   \n",
      "3  high_school_us_history_0  high_school_us_history   \n",
      "4  high_school_us_history_1  high_school_us_history   \n",
      "5  high_school_us_history_1  high_school_us_history   \n",
      "6  high_school_us_history_1  high_school_us_history   \n",
      "7  high_school_us_history_1  high_school_us_history   \n",
      "\n",
      "                                            question choice_str  choice_index  \\\n",
      "0  This question refers to the following informat...          A             0   \n",
      "1  This question refers to the following informat...          B             1   \n",
      "2  This question refers to the following informat...          C             2   \n",
      "3  This question refers to the following informat...          D             3   \n",
      "4  This question refers to the following informat...          A             0   \n",
      "5  This question refers to the following informat...          B             1   \n",
      "6  This question refers to the following informat...          C             2   \n",
      "7  This question refers to the following informat...          D             3   \n",
      "\n",
      "                                         choice_text   probability  aps_score  \\\n",
      "0  The United States was granted the territory in...  6.029919e-06   0.000000   \n",
      "1  The United States bought it from the Native Am...  1.009255e-05   0.000006   \n",
      "2  U.S. settlers were the first to arrive in the ...  9.992287e-01   0.000771   \n",
      "3  Great Britain ceded it to the United States as...  7.551559e-04   0.000016   \n",
      "4  one of the most religiously diverse colonies i...  1.000000e+00   0.000000   \n",
      "5  one of the least religiously diverse colonies ...  3.721979e-44   0.000000   \n",
      "6  notorious for witch hunting and popular supers...  3.721979e-44   0.000000   \n",
      "7  known for its hostility to traditional religio...  3.721979e-44   0.000000   \n",
      "\n",
      "   is_ground_truth  \n",
      "0            False  \n",
      "1            False  \n",
      "2            False  \n",
      "3             True  \n",
      "4             True  \n",
      "5            False  \n",
      "6            False  \n",
      "7            False  \n",
      "\n",
      "--- Example: Scores for one question ---\n",
      "       question_id choice_str  probability     aps_score  is_ground_truth\n",
      "1756  philosophy_0          A     0.000577  8.688158e-05            False\n",
      "1757  philosophy_0          B     0.000035  1.110223e-16            False\n",
      "1758  philosophy_0          C     0.999336  6.638737e-04             True\n",
      "1759  philosophy_0          D     0.000052  3.535156e-05            False\n"
     ]
    }
   ],
   "source": [
    "# 转换为 Pandas DataFrame\n",
    "df_scores = pd.DataFrame(results_list)\n",
    "\n",
    "# 保存到 Parquet 文件 (比 CSV 更高效)\n",
    "# 修复 PyArrow 文件系统注册错误\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 使用绝对路径\n",
    "output_filename = os.path.abspath(\"mmlu_with_aps_scores.parquet\")\n",
    "\n",
    "# 尝试使用 PyArrow 的低级 API 直接写入，避免文件系统注册问题\n",
    "try:\n",
    "    # 方法1: 使用 pyarrow.parquet.write_table 直接写入\n",
    "    table = pa.Table.from_pandas(df_scores, preserve_index=False)\n",
    "    pq.write_table(table, output_filename)\n",
    "except Exception as e:\n",
    "    if \"ArrowKeyError\" in str(type(e).__name__) or \"already registered\" in str(e):\n",
    "        # 方法2: 使用 pandas 的 to_parquet，但指定 engine\n",
    "        try:\n",
    "            df_scores.to_parquet(output_filename, index=False, engine='pyarrow')\n",
    "        except:\n",
    "            # 方法3: 尝试 fastparquet\n",
    "            try:\n",
    "                df_scores.to_parquet(output_filename, index=False, engine='fastparquet')\n",
    "            except ImportError:\n",
    "                # 方法4: 如果都失败，使用 CSV 作为后备\n",
    "                csv_filename = output_filename.replace('.parquet', '.csv')\n",
    "                print(f\"Warning: PyArrow filesystem registry error. Saving as CSV instead: {csv_filename}\")\n",
    "                df_scores.to_csv(csv_filename, index=False)\n",
    "                output_filename = csv_filename\n",
    "            except Exception as e2:\n",
    "                csv_filename = output_filename.replace('.parquet', '.csv')\n",
    "                print(f\"Warning: Could not save as Parquet ({e2}). Saving as CSV instead: {csv_filename}\")\n",
    "                df_scores.to_csv(csv_filename, index=False)\n",
    "                output_filename = csv_filename\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# 计算每个问题的选项数量（通过统计每个question_id的行数）\n",
    "questions_per_row = df_scores.groupby('question_id').size()\n",
    "avg_choices = questions_per_row.mean()\n",
    "print(f\"Successfully processed {len(df_scores)} rows ({len(questions_per_row)} questions).\")\n",
    "print(f\"Average number of choices per question: {avg_choices:.1f}\")\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "\n",
    "# --- 验证一下我们的数据 ---\n",
    "print(\"\\n--- DataFrame Head ---\")\n",
    "print(df_scores.head(8))\n",
    "\n",
    "print(\"\\n--- Example: Scores for one question ---\")\n",
    "print(df_scores[df_scores['question_id'] == 'philosophy_0'][\n",
    "    ['question_id', 'choice_str', 'probability', 'aps_score', 'is_ground_truth']\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal-factual-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}