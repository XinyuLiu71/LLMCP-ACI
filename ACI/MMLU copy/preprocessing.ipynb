{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b12f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API 密钥已从代码中加载\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 从环境变量读取 OpenAI API 密钥\n",
    "# 密钥应该已经在 ~/.bashrc 中设置，或者通过 export OPENAI_API_KEY=... 设置\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set. Please set it before running this notebook.\")\n",
    "else:\n",
    "    print(\"✓ OpenAI API 密钥已从环境变量加载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23b7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai transformers datasets accelerate torch pandas pyarrow tqdm\n",
    "# 'accelerate' 是为了更快地加载和运行模型\n",
    "# 'pyarrow' 是为了将 DataFrame 保存为 parquet 格式\n",
    "# 'openai' 用于 GPT-3.5 API\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 检查是否有可用的 GPU (在 Colab 或本地)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 初始化 OpenAI 客户端 (需要设置 OPENAI_API_KEY 环境变量)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be85c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-3.5-turbo via OpenAI API\n",
      "Model ready to use.\n"
     ]
    }
   ],
   "source": [
    "# 使用 GPT-3.5 Turbo\n",
    "MODEL_ID = \"gpt-3.5-turbo\"\n",
    "\n",
    "# 注意：GPT-3.5 通过 OpenAI API 使用，不需要本地加载模型\n",
    "# 确保已设置 OPENAI_API_KEY 环境变量\n",
    "print(f\"Using model: {MODEL_ID} via OpenAI API\")\n",
    "print(\"Model ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90af7d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all available MMLU subjects...\n",
      "Found 59 subjects in MMLU dataset\n",
      "After filtering out 'all', 58 subjects remain\n",
      "Selected first 2 subjects for processing:\n",
      "Subjects: ['abstract_algebra', 'anatomy']\n",
      "Loaded 100 questions for subject: abstract_algebra\n",
      "Loaded 135 questions for subject: anatomy\n",
      "\n",
      "Successfully loaded 2 subjects out of 2 selected subjects\n"
     ]
    }
   ],
   "source": [
    "# 获取 MMLU 数据集的所有可用主题\n",
    "# 先获取所有主题，排除 'all'，然后选择前20个进行标注\n",
    "print(\"Fetching all available MMLU subjects...\")\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "# 获取所有可用的配置（主题）\n",
    "all_subjects = get_dataset_config_names(\"cais/mmlu\")\n",
    "print(f\"Found {len(all_subjects)} subjects in MMLU dataset\")\n",
    "\n",
    "# 排除 'all' 这个subject\n",
    "filtered_subjects = [s for s in all_subjects if s != 'all']\n",
    "print(f\"After filtering out 'all', {len(filtered_subjects)} subjects remain\")\n",
    "\n",
    "# 只选择前20个subject进行标注\n",
    "SUBJECTS = filtered_subjects[:2]\n",
    "print(f\"Selected first {len(SUBJECTS)} subjects for processing:\")\n",
    "print(f\"Subjects: {SUBJECTS}\")\n",
    "\n",
    "# 用于存储所有加载数据的字典\n",
    "mmlu_data = {}\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    # MMLU 的 \"test\" 集是有标签的，\"validation\" 集是无标签的（用于官方提交）\n",
    "    # 所以我们加载 \"test\" 集\n",
    "    try:\n",
    "        dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
    "        mmlu_data[subject] = dataset\n",
    "        print(f\"Loaded {len(dataset)} questions for subject: {subject}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {subject}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(mmlu_data)} subjects out of {len(SUBJECTS)} selected subjects\")\n",
    "\n",
    "# MMLU 的选项\n",
    "CHOICES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e71f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mmlu_prompt(sample, subject_name):\n",
    "    \"\"\"\n",
    "    将 MMLU 的一行数据格式化为 zero-shot CoT prompt。\n",
    "    \"\"\"\n",
    "    subject_formatted = subject_name.replace(\"_\", \" \")\n",
    "    question = sample['question']\n",
    "    \n",
    "    # 组合选项\n",
    "    options = \"\"\n",
    "    for i, choice in enumerate(sample['choices']):\n",
    "        options += f\"{CHOICES[i]}. {choice}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"The following is a multiple-choice question about {subject_formatted}. Please choose the single most likely answer.\n",
    "\n",
    "Question: {question}\n",
    "{options}\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_choice_probabilities(prompt, model_id, client, num_choices=None):\n",
    "    \"\"\"\n",
    "    给定一个 prompt，计算模型对选项的概率。\n",
    "    使用 OpenAI API 的 logprobs 功能。\n",
    "    \n",
    "    Args:\n",
    "        prompt: 输入提示\n",
    "        model_id: 模型ID\n",
    "        client: OpenAI客户端\n",
    "        num_choices: 选项数量（如果为None，则使用CHOICES的长度）\n",
    "    \"\"\"\n",
    "    # 确定实际使用的选项数量\n",
    "    if num_choices is None:\n",
    "        num_choices = len(CHOICES)\n",
    "    actual_choices = CHOICES[:num_choices]\n",
    "    \n",
    "    # 1. 准备选项 token (GPT-3.5 通常使用 \" A\", \" B\", \" C\", \" D\" 等格式)\n",
    "    choice_tokens = [f\" {choice}\" for choice in actual_choices]\n",
    "    \n",
    "    # 2. 调用 OpenAI API 获取 logprobs (带重试机制)\n",
    "    import time\n",
    "    max_retries = 3\n",
    "    retry_delay = 1  # 初始延迟（秒）\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                logprobs=True,  # 启用 logprobs\n",
    "                top_logprobs=20,  # 获取 top 20 的 logprobs\n",
    "                max_tokens=1,  # 只生成一个 token\n",
    "                temperature=0  # 使用确定性输出\n",
    "            )\n",
    "            break  # 成功则跳出重试循环\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                # 如果是速率限制错误，等待更长时间\n",
    "                if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                    wait_time = retry_delay * (2 ** attempt)  # 指数退避\n",
    "                    print(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    time.sleep(retry_delay * (2 ** attempt))\n",
    "                continue\n",
    "            else:\n",
    "                # 最后一次尝试也失败，返回均匀分布\n",
    "                print(f\"Error calling OpenAI API after {max_retries} attempts: {e}\")\n",
    "                return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    if response is None:\n",
    "        return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    # 3. 获取第一个（也是唯一的）token 的 logprobs\n",
    "    if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
    "        token_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "        # 创建一个字典，将 token 文本映射到 logprob\n",
    "        logprob_dict = {item.token: item.logprob for item in token_logprobs}\n",
    "    else:\n",
    "        logprob_dict = {}\n",
    "    \n",
    "    # 4. 提取每个选项的 logprob\n",
    "    choice_logprobs = []\n",
    "    for choice_token in choice_tokens:\n",
    "        choice_letter = choice_token.strip()  # 获取字母部分 (A, B, C, D, E, F)\n",
    "        logprob = None\n",
    "        \n",
    "        # 尝试多种可能的 token 格式\n",
    "        # 1. 带前导空格的格式: \" A\", \" B\", \" C\", etc.\n",
    "        # 注意: choice_token 已经是 \" A\" 格式，所以直接使用\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(choice_token, None)\n",
    "        \n",
    "        # 2. 不带空格的格式: \"A\", \"B\", \"C\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(choice_letter, None)\n",
    "        \n",
    "        # 3. 带点号的格式: \"A.\", \"B.\", \"C.\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(f\"{choice_letter}.\", None)\n",
    "        \n",
    "        # 4. 带前导空格和点号的格式: \" A.\", \" B.\", \" C.\", etc.\n",
    "        if logprob is None:\n",
    "            logprob = logprob_dict.get(f\" {choice_letter}.\", None)\n",
    "        \n",
    "        # 5. 规范化匹配：去除所有空格和标点后比较\n",
    "        if logprob is None:\n",
    "            for token, lp in logprob_dict.items():\n",
    "                # 规范化 token：去除空格、点号等，只保留字母\n",
    "                normalized_token = ''.join(c for c in token if c.isalpha())\n",
    "                if normalized_token == choice_letter:\n",
    "                    logprob = lp\n",
    "                    break\n",
    "        \n",
    "        # 6. 大小写不敏感匹配\n",
    "        if logprob is None:\n",
    "            for token, lp in logprob_dict.items():\n",
    "                normalized_token = ''.join(c for c in token if c.isalpha())\n",
    "                if normalized_token.upper() == choice_letter.upper():\n",
    "                    logprob = lp\n",
    "                    break\n",
    "        \n",
    "        if logprob is None:\n",
    "            # 如果找不到，使用一个很小的值\n",
    "            logprob = -100.0\n",
    "            # 只在第一次找不到时打印调试信息\n",
    "            if len(choice_logprobs) == 0:  # 只在第一个选项找不到时打印\n",
    "                print(f\"Warning: Could not find logprob for choice token '{choice_token}'\")\n",
    "                print(f\"Available tokens (top 10): {list(logprob_dict.keys())[:10]}\")\n",
    "        \n",
    "        choice_logprobs.append(logprob)\n",
    "    \n",
    "    # 5. 将 logprobs 转换为 logits (logprobs 已经是 log 概率)\n",
    "    choice_logits = np.array(choice_logprobs)\n",
    "    \n",
    "    # 6. 应用 softmax 得到概率分布\n",
    "    # 为了避免数值不稳定，减去最大值\n",
    "    choice_logits_shifted = choice_logits - np.max(choice_logits)\n",
    "    exp_logits = np.exp(choice_logits_shifted)\n",
    "    choice_probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    return choice_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279e44e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test S(A): 0.42\n",
      "Test S(B): 0.8200000000000001\n",
      "Test S(C): 0.92\n",
      "Test S(D): 0.9700000000000001\n",
      "Test S(E): 0.9900000000000001\n",
      "Test S(F): 1.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_aps_score(probs, choice_index):\n",
    "    \"\"\"\n",
    "    为 *一个* 假设的答案 (choice_index) 计算 APS 不一致性分数。\n",
    "    S(X, y) = 1 - (所有 P_j >= P_y 的 P_j 的总和)\n",
    "    \n",
    "    Args:\n",
    "    - probs (np.array): 概率数组，例如 [P(A), P(B), P(C), P(D), P(E), P(F)]\n",
    "    - choice_index (int): 我们正在计算分数的那个选项 (0=A, 1=B, 2=C, 3=D, 4=E, 5=F)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 获取我们正在打分的这个选项的概率\n",
    "    prob_y = probs[choice_index]\n",
    "    \n",
    "    # 2. 找到所有概率 >= prob_y 的选项\n",
    "    indices_to_sum = np.where(probs >= prob_y)[0]\n",
    "    \n",
    "    # 3. 把它们的概率加起来\n",
    "    # 为了处理浮点数精度问题，我们应该比较 probs >= prob_y - 1e-9\n",
    "    prob_sum = 0\n",
    "    for idx in indices_to_sum:\n",
    "        # 再次检查，避免浮点数问题\n",
    "        if probs[idx] >= prob_y - 1e-9:\n",
    "            prob_sum += probs[idx]\n",
    "            \n",
    "    # 4. APS 分数\n",
    "    score = prob_sum\n",
    "    \n",
    "    return score\n",
    "\n",
    "# --- 快速测试一下我们的计分函数 ---\n",
    "test_probs = np.array([0.42, 0.40, 0.10, 0.05, 0.02, 0.01])\n",
    "# 选项 A: S(A) = 1 - P(A) = 1 - 0.42 = 0.58\n",
    "# 选项 B: S(B) = 1 - (P(A) + P(B)) = 1 - (0.42 + 0.40) = 0.18\n",
    "# 选项 C: S(C) = 1 - (P(A) + P(B) + P(C)) = 1 - (0.45 + 0.40 + 0.10) = 0.05\n",
    "# 选项 D: S(D) = 1 - (P(A) + P(B) + P(C) + P(D)) = 1 - 1.0 = 0.0\n",
    "\n",
    "print(f\"Test S(A): {calculate_aps_score(test_probs, 0)}\") # 应该约等于 0.58\n",
    "print(f\"Test S(B): {calculate_aps_score(test_probs, 1)}\") # 应该约等于 0.18\n",
    "print(f\"Test S(C): {calculate_aps_score(test_probs, 2)}\") # 应该约等于 0.08\n",
    "print(f\"Test S(D): {calculate_aps_score(test_probs, 3)}\") # 应该约等于 0.03\n",
    "print(f\"Test S(E): {calculate_aps_score(test_probs, 4)}\") # 应该约等于 0.01\n",
    "print(f\"Test S(F): {calculate_aps_score(test_probs, 5)}\") # 应该约等于 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e5e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject: abstract_algebra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject: anatomy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [01:18<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_list = [] # 用于存储我们所有数据的列表\n",
    "\n",
    "# 遍历我们加载的每个 MMLU 主题\n",
    "for subject_name, dataset in mmlu_data.items():\n",
    "    print(f\"\\nProcessing subject: {subject_name}...\")\n",
    "    \n",
    "    # 遍历该主题中的所有问题\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        \n",
    "        # 1. 格式化 prompt\n",
    "        prompt = format_mmlu_prompt(sample, subject_name)\n",
    "        \n",
    "        # 2. 获取实际选项数量（MMLU问题可能有不同数量的选项）\n",
    "        num_choices = len(sample['choices'])\n",
    "        \n",
    "        # 3. 获取概率分布 [P(A), P(B), P(C), ...]\n",
    "        try:\n",
    "            probabilities = get_choice_probabilities(prompt, MODEL_ID, client, num_choices=num_choices)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # 4. 获取标准答案\n",
    "        ground_truth_label = sample['answer'] # 这是一个 0 到 num_choices-1 的索引\n",
    "        \n",
    "        # 5. 为 *每一个* 选项计算 APS 分数\n",
    "        for j in range(num_choices):\n",
    "            \n",
    "            aps_score = calculate_aps_score(probabilities, j)\n",
    "            \n",
    "            # 6. 结构化保存\n",
    "            row = {\n",
    "                \"question_id\": f\"{subject_name}_{i}\",\n",
    "                \"subject\": subject_name,\n",
    "                \"question\": sample['question'],\n",
    "                \"choice_str\": CHOICES[j],          # A, B, C, D, E, or F\n",
    "                \"choice_index\": j,\n",
    "                \"choice_text\": sample['choices'][j],\n",
    "                \"probability\": probabilities[j],   # 模型对这个选项的原始概率\n",
    "                \"aps_score\": aps_score,            # 这个选项的 APS 不一致性分数\n",
    "                \"is_ground_truth\": (j == ground_truth_label) # 这是一个 bool 值\n",
    "            }\n",
    "            results_list.append(row)\n",
    "\n",
    "print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b635783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: PyArrow filesystem registry error. Saving as CSV instead: /egr/research-hintlab/liuxin73/projects/conformal-factual-lm/ACI/MMLU copy/mmlu_with_aps_scores.csv\n",
      "Successfully processed 940 rows (235 questions).\n",
      "Average number of choices per question: 4.0\n",
      "Data saved to /egr/research-hintlab/liuxin73/projects/conformal-factual-lm/ACI/MMLU copy/mmlu_with_aps_scores.csv\n",
      "\n",
      "--- DataFrame Head ---\n",
      "          question_id           subject  \\\n",
      "0  abstract_algebra_0  abstract_algebra   \n",
      "1  abstract_algebra_0  abstract_algebra   \n",
      "2  abstract_algebra_0  abstract_algebra   \n",
      "3  abstract_algebra_0  abstract_algebra   \n",
      "4  abstract_algebra_1  abstract_algebra   \n",
      "5  abstract_algebra_1  abstract_algebra   \n",
      "6  abstract_algebra_1  abstract_algebra   \n",
      "7  abstract_algebra_1  abstract_algebra   \n",
      "\n",
      "                                            question choice_str  choice_index  \\\n",
      "0  Find the degree for the given field extension ...          A             0   \n",
      "1  Find the degree for the given field extension ...          B             1   \n",
      "2  Find the degree for the given field extension ...          C             2   \n",
      "3  Find the degree for the given field extension ...          D             3   \n",
      "4  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...          A             0   \n",
      "5  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...          B             1   \n",
      "6  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...          C             2   \n",
      "7  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...          D             3   \n",
      "\n",
      "  choice_text  probability  aps_score  is_ground_truth  \n",
      "0           0     0.000682   1.000000            False  \n",
      "1           4     0.234496   0.976007             True  \n",
      "2           2     0.023311   0.999318            False  \n",
      "3           6     0.741511   0.741511            False  \n",
      "4           8     0.022888   1.000000            False  \n",
      "5           2     0.051379   0.977112            False  \n",
      "6          24     0.784832   0.784832             True  \n",
      "7         120     0.140902   0.925733            False  \n",
      "\n",
      "--- Example: Scores for one question ---\n",
      "Empty DataFrame\n",
      "Columns: [question_id, choice_str, probability, aps_score, is_ground_truth]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 转换为 Pandas DataFrame\n",
    "df_scores = pd.DataFrame(results_list)\n",
    "\n",
    "# 保存到 Parquet 文件 (比 CSV 更高效)\n",
    "# 修复 PyArrow 文件系统注册错误\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 使用绝对路径\n",
    "output_filename = os.path.abspath(\"mmlu_with_aps_scores.parquet\")\n",
    "\n",
    "# 尝试使用 PyArrow 的低级 API 直接写入，避免文件系统注册问题\n",
    "try:\n",
    "    # 方法1: 使用 pyarrow.parquet.write_table 直接写入\n",
    "    table = pa.Table.from_pandas(df_scores, preserve_index=False)\n",
    "    pq.write_table(table, output_filename)\n",
    "except Exception as e:\n",
    "    if \"ArrowKeyError\" in str(type(e).__name__) or \"already registered\" in str(e):\n",
    "        # 方法2: 使用 pandas 的 to_parquet，但指定 engine\n",
    "        try:\n",
    "            df_scores.to_parquet(output_filename, index=False, engine='pyarrow')\n",
    "        except:\n",
    "            # 方法3: 尝试 fastparquet\n",
    "            try:\n",
    "                df_scores.to_parquet(output_filename, index=False, engine='fastparquet')\n",
    "            except ImportError:\n",
    "                # 方法4: 如果都失败，使用 CSV 作为后备\n",
    "                csv_filename = output_filename.replace('.parquet', '.csv')\n",
    "                print(f\"Warning: PyArrow filesystem registry error. Saving as CSV instead: {csv_filename}\")\n",
    "                df_scores.to_csv(csv_filename, index=False)\n",
    "                output_filename = csv_filename\n",
    "            except Exception as e2:\n",
    "                csv_filename = output_filename.replace('.parquet', '.csv')\n",
    "                print(f\"Warning: Could not save as Parquet ({e2}). Saving as CSV instead: {csv_filename}\")\n",
    "                df_scores.to_csv(csv_filename, index=False)\n",
    "                output_filename = csv_filename\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# 计算每个问题的选项数量（通过统计每个question_id的行数）\n",
    "questions_per_row = df_scores.groupby('question_id').size()\n",
    "avg_choices = questions_per_row.mean()\n",
    "print(f\"Successfully processed {len(df_scores)} rows ({len(questions_per_row)} questions).\")\n",
    "print(f\"Average number of choices per question: {avg_choices:.1f}\")\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "\n",
    "# --- 验证一下我们的数据 ---\n",
    "print(\"\\n--- DataFrame Head ---\")\n",
    "print(df_scores.head(8))\n",
    "\n",
    "print(\"\\n--- Example: Scores for one question ---\")\n",
    "print(df_scores[df_scores['question_id'] == 'philosophy_0'][\n",
    "    ['question_id', 'choice_str', 'probability', 'aps_score', 'is_ground_truth']\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal-factual-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
